{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0caaaa2e-3eb8-47df-aed8-7dd3e78791cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 09:18:19.814146: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-04 09:18:19.844860: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-04 09:18:20.529334: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import ast\n",
    "import json\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1aa793-f256-45b8-a738-e4c5b8b7d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d301fe-d4a4-4d32-a60f-e91f255cdb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08271b-8006-40e3-8809-8bdba3648d92",
   "metadata": {},
   "source": [
    "### Load the preprocessed video corpus and frames metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5a4993-3020-44aa-ba5a-db9d3bd30b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_corpus = pd.read_csv('data/final_video_corpus.csv')\n",
    "video_corpus = pd.read_csv('data/final_video_corpus.csv')\n",
    "video_corpus['PaddedSequence'] = video_corpus['PaddedSequence'].apply(json.loads)\n",
    "frames_metadata = pd.read_csv('data/frames_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02efb7c7-6cbc-45a3-beda-0b4f084041d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Description</th>\n",
       "      <th>PaddedSequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mv89psg6zh4_33_46</td>\n",
       "      <td>&lt;bos&gt; a bird is bathing in a sink &lt;eos&gt;</td>\n",
       "      <td>[3, 2, 253, 5, 554, 9, 2, 465, 4, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mv89psg6zh4_33_46</td>\n",
       "      <td>&lt;bos&gt; a bird is splashing around under a runni...</td>\n",
       "      <td>[3, 2, 253, 5, 1, 81, 318, 2, 47, 903, 4, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mv89psg6zh4_33_46</td>\n",
       "      <td>&lt;bos&gt; a bird is bathing in a sink &lt;eos&gt;</td>\n",
       "      <td>[3, 2, 253, 5, 554, 9, 2, 465, 4, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mv89psg6zh4_33_46</td>\n",
       "      <td>&lt;bos&gt; a faucet is running while a bird stands ...</td>\n",
       "      <td>[3, 2, 903, 5, 47, 90, 2, 253, 1087, 9, 6, 465...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mv89psg6zh4_33_46</td>\n",
       "      <td>&lt;bos&gt; a bird is playing in a sink with running...</td>\n",
       "      <td>[3, 2, 253, 5, 11, 9, 2, 465, 15, 47, 32, 4, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             VideoID                                        Description  \\\n",
       "0  mv89psg6zh4_33_46            <bos> a bird is bathing in a sink <eos>   \n",
       "1  mv89psg6zh4_33_46  <bos> a bird is splashing around under a runni...   \n",
       "2  mv89psg6zh4_33_46            <bos> a bird is bathing in a sink <eos>   \n",
       "3  mv89psg6zh4_33_46  <bos> a faucet is running while a bird stands ...   \n",
       "4  mv89psg6zh4_33_46  <bos> a bird is playing in a sink with running...   \n",
       "\n",
       "                                      PaddedSequence  \n",
       "0  [3, 2, 253, 5, 554, 9, 2, 465, 4, 0, 0, 0, 0, ...  \n",
       "1  [3, 2, 253, 5, 1, 81, 318, 2, 47, 903, 4, 0, 0...  \n",
       "2  [3, 2, 253, 5, 554, 9, 2, 465, 4, 0, 0, 0, 0, ...  \n",
       "3  [3, 2, 903, 5, 47, 90, 2, 253, 1087, 9, 6, 465...  \n",
       "4  [3, 2, 253, 5, 11, 9, 2, 465, 15, 47, 32, 4, 0...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15146784-b6dd-4900-adad-8957c9778799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Key_Frames</th>\n",
       "      <th>Total_Frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4wsuPCjDBc_5_15</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-7KMZQEsJW4_205_208</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8y1Q0rA3n8_108_115</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8y1Q0rA3n8_95_102</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9CUm-2cui8_39_44</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               VideoID  Key_Frames  Total_Frames\n",
       "0     -4wsuPCjDBc_5_15           0            50\n",
       "1  -7KMZQEsJW4_205_208           0            50\n",
       "2  -8y1Q0rA3n8_108_115           3            50\n",
       "3   -8y1Q0rA3n8_95_102           4            50\n",
       "4    -9CUm-2cui8_39_44           2            50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808c850-f9a3-4f22-b34e-0d765d80b0d9",
   "metadata": {},
   "source": [
    "#### Expect no duplicates in the frames metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38faae59-8a2c-40ea-8832-2f1e89f551c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicates present in the frames_metadata\n"
     ]
    }
   ],
   "source": [
    "if(frames_metadata.duplicated(subset=['VideoID']).any()):\n",
    "    print(\"There are duplicates present in the frames_metadata\")\n",
    "else:\n",
    "    print(\"There are no duplicates present in the frames_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6f8ce-e884-4e06-824f-9729d04697cd",
   "metadata": {},
   "source": [
    "### Perform train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "868c6cce-bd42-4ce5-844e-218c1badca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(frames_metadata['VideoID'].unique(), test_size=0.2, random_state=42)\n",
    "\n",
    "train_corpus = video_corpus[video_corpus['VideoID'].isin(train_ids)].reset_index(drop=True)\n",
    "test_corpus = video_corpus[video_corpus['VideoID'].isin(test_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70692bae-6964-4161-9fbd-cca96a7f9cc7",
   "metadata": {},
   "source": [
    "### Load visual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14e764f-ecf3-4596-a890-77db507fa0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dir = 'data/ExtractedFeatures/' \n",
    "x_data = {}\n",
    "\n",
    "for video_id in os.listdir(features_dir):\n",
    "    subdir = os.path.join(features_dir, video_id)\n",
    "    if not os.path.isdir(subdir):\n",
    "        continue\n",
    "    \n",
    "    feature_path = os.path.join(subdir, f\"{video_id}.npy\")\n",
    "\n",
    "    if os.path.exists(feature_path):\n",
    "        x_data[video_id] = np.load(feature_path)\n",
    "    else:\n",
    "        print(f\"Warning: Feature file missing for {video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1e3822-955e-42c8-a0c6-9c8520d62ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (50, 4096)\n"
     ]
    }
   ],
   "source": [
    "video_id = list(x_data.keys())[0]\n",
    "features = x_data[video_id]\n",
    "print(\"Feature shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d72c101-9e80-4f2d-a7f5-543a0d274611",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_video_ids = set(x_data.keys())\n",
    "train_corpus = train_corpus[train_corpus['VideoID'].isin(valid_video_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c96d2-5ec8-436a-adc8-7b9213ddb9d3",
   "metadata": {},
   "source": [
    "### Model Architecture and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28893ea4-b480-41b7-91b2-2cfe761db32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # features: (batch, seq_len, input_size)\n",
    "        outputs, (hidden, cell) = self.lstm(features)\n",
    "        return hidden, cell\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_seq, hidden, cell):\n",
    "        # input_seq: (batch, seq_len)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        outputs = self.linear(outputs)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class VideoCaptioningModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, features, input_seq):\n",
    "        hidden, cell = self.encoder(features)\n",
    "        outputs, _, _ = self.decoder(input_seq, hidden, cell)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8dd24e2-9993-4e27-8622-6e518b2615b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCaptionDataset(Dataset):\n",
    "    def __init__(self, corpus_df, x_data, tokenizer):\n",
    "        \"\"\"\n",
    "        corpus_df: DataFrame with columns 'VideoID' and 'PaddedSequence' (list of ints)\n",
    "        x_data: dict mapping VideoID -> np.array (variable length feature seqs)\n",
    "        tokenizer: tokenizer object (optional, for vocab size etc)\n",
    "        \"\"\"\n",
    "        self.corpus_df = corpus_df\n",
    "        self.x_data = x_data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.corpus_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.corpus_df.iloc[idx]\n",
    "        video_id = row['VideoID']\n",
    "        \n",
    "        # Features: shape (seq_len, feature_dim)\n",
    "        features = torch.tensor(self.x_data[video_id], dtype=torch.float)\n",
    "        \n",
    "        # Caption sequence, e.g. [<bos>, w1, w2, ..., <eos>, 0, 0, ...]\n",
    "        caption = row['PaddedSequence']  # list of ints\n",
    "        \n",
    "        caption = torch.tensor(caption, dtype=torch.long)\n",
    "        \n",
    "        # Prepare inputs and targets for decoder:\n",
    "        input_seq = caption[:-1]\n",
    "        target_seq = caption[1:]\n",
    "        \n",
    "        return features, input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec9ab93-207f-48e4-b612-b9f78f56630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    features, input_seqs, target_seqs = zip(*batch)\n",
    "    \n",
    "    # Pad features along seq_len dim\n",
    "    max_len = max(f.shape[0] for f in features)\n",
    "    padded_features = []\n",
    "    for f in features:\n",
    "        pad_len = max_len - f.shape[0]\n",
    "        if pad_len > 0:\n",
    "            padding = torch.zeros(pad_len, f.shape[1], device=f.device)\n",
    "            f = torch.cat([f, padding], dim=0)\n",
    "        padded_features.append(f)\n",
    "    padded_features = torch.stack(padded_features)  # (batch, max_len, feat_dim)\n",
    "    \n",
    "    # Pad input and target captions (batch_first=True)\n",
    "    input_seqs = rnn_utils.pad_sequence(input_seqs, batch_first=True, padding_value=0)\n",
    "    target_seqs = rnn_utils.pad_sequence(target_seqs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return padded_features, input_seqs, target_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2490be43-abcd-4982-be57-542efb7b5c6a",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62761f0f-df88-44d0-b120-18c333654674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e4481e-9f04-481f-bb4a-e3765df24a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoCaptionDataset(train_corpus, x_data, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd7199-d937-4009-96b3-4f085ddf2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4096\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "encoder = EncoderLSTM(input_size, hidden_size).to(device)\n",
    "decoder = DecoderLSTM(embed_size, hidden_size, vocab_size).to(device)\n",
    "model = VideoCaptioningModel(encoder, decoder).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "817c576f-3b2b-40a8-8eca-201530505ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, input_seq, target_seq in dataloader:\n",
    "        features = features.to(device)\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features, input_seq)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # Flatten outputs and targets for loss\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        target_seq = target_seq.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, target_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edcfd8b9-fb1b-4b03-87f7-619230c518f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.1743\n",
      "Epoch 2, Loss: 2.2833\n",
      "Epoch 3, Loss: 1.9604\n",
      "Epoch 4, Loss: 1.7886\n",
      "Epoch 5, Loss: 1.6800\n",
      "Epoch 6, Loss: 1.6017\n",
      "Epoch 7, Loss: 1.5378\n",
      "Epoch 8, Loss: 1.4850\n",
      "Epoch 9, Loss: 1.4391\n",
      "Epoch 10, Loss: 1.3978\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, input_seq, target_seq in dataloader:\n",
    "        features = features.to(device)\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features, input_seq)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # Flatten outputs and targets for loss\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        target_seq = target_seq.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, target_seq)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdfe11e0-eeec-4a47-a514-cda461c2d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 1.3647\n",
      "Epoch 12, Loss: 1.3234\n",
      "Epoch 13, Loss: 1.2896\n",
      "Epoch 14, Loss: 1.2567\n",
      "Epoch 15, Loss: 1.2262\n",
      "Epoch 16, Loss: 1.1980\n",
      "Epoch 17, Loss: 1.1695\n",
      "Epoch 18, Loss: 1.1432\n",
      "Epoch 19, Loss: 1.1180\n",
      "Epoch 20, Loss: 1.0922\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('video_caption_model_1.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(10, 20):  \n",
    "    loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "    # torch.save(model.state_dict(), f'video_caption_model_epoch{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ff39a52-845a-4a10-be24-170404a7951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 1.0733\n",
      "Epoch 22, Loss: 1.0465\n",
      "Epoch 23, Loss: 1.0235\n",
      "Epoch 24, Loss: 1.0012\n",
      "Epoch 25, Loss: 0.9806\n",
      "Epoch 26, Loss: 0.9606\n",
      "Epoch 27, Loss: 0.9412\n",
      "Epoch 28, Loss: 0.9224\n",
      "Epoch 29, Loss: 0.9044\n",
      "Epoch 30, Loss: 0.8874\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('video_caption_model_2.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(20, 30):  \n",
    "    loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6944b028-dadd-479a-a7a3-98a0460f4c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 0.8756\n",
      "Epoch 32, Loss: 0.8562\n",
      "Epoch 33, Loss: 0.8410\n",
      "Epoch 34, Loss: 0.8262\n",
      "Epoch 35, Loss: 0.8122\n",
      "Epoch 36, Loss: 0.8000\n",
      "Epoch 37, Loss: 0.7876\n",
      "Epoch 38, Loss: 0.7758\n",
      "Epoch 39, Loss: 0.7645\n",
      "Epoch 40, Loss: 0.7542\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('video_caption_model_3.pth', map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(30, 40):  \n",
    "    loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e91b8f68-c262-4fb5-af33-4910752d176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'video_caption_model_4.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54f50b-b57b-4df0-8f8a-a908c4bd2570",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e57888-adb9-4b43-8130-019a1b6b80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, features, tokenizer, max_len=20):\n",
    "    model.eval()\n",
    "    features = features.unsqueeze(0).to(device)  # add batch dimension\n",
    "    \n",
    "    bos_token = 'bos'  # no angle brackets\n",
    "    eos_token = 'eos'\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(features)\n",
    "        input_seq = torch.tensor([tokenizer.word_index[bos_token]], device=device).unsqueeze(0)\n",
    "        generated = []\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = model.decoder(input_seq, hidden, cell)\n",
    "            output = output.squeeze(1)\n",
    "            predicted_id = output.argmax(1).item()\n",
    "            \n",
    "            if predicted_id == tokenizer.word_index.get(eos_token, 0):\n",
    "                break\n",
    "                \n",
    "            generated.append(predicted_id)\n",
    "            input_seq = torch.tensor([predicted_id], device=device).unsqueeze(0)\n",
    "    \n",
    "    inv_map = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    caption_words = [inv_map.get(i, '<unk>') for i in generated]\n",
    "    \n",
    "    return ' '.join(caption_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9dedbe4e-a82f-4adf-a450-6937db12f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random video ID: 1N_Ic2pBM1o_2_23\n",
      "Generated caption: a young man is playing the guitar\n"
     ]
    }
   ],
   "source": [
    "input_size = 4096\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = VideoCaptioningModel(EncoderLSTM(input_size, hidden_size), DecoderLSTM(embed_size, hidden_size, vocab_size)).to(device)\n",
    "model.load_state_dict(torch.load('video_caption_model_4.pth', map_location=device))\n",
    "\n",
    "random_video_id = random.choice(test_corpus['VideoID'].unique())\n",
    "print(f\"Random video ID: {random_video_id}\")\n",
    "\n",
    "features = torch.tensor(x_data[random_video_id], dtype=torch.float)\n",
    "caption = generate_caption(model, features, tokenizer)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1cac91e-6fdf-4c6c-9fb6-7697bc09155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random video ID: _6OTzzK7t9Y_158_170\n",
      "Generated caption: a man is playing a piano\n"
     ]
    }
   ],
   "source": [
    "input_size = 4096\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = VideoCaptioningModel(EncoderLSTM(input_size, hidden_size), DecoderLSTM(embed_size, hidden_size, vocab_size)).to(device)\n",
    "model.load_state_dict(torch.load('video_caption_model_4.pth', map_location=device))\n",
    "\n",
    "random_video_id = random.choice(test_corpus['VideoID'].unique())\n",
    "print(f\"Random video ID: {random_video_id}\")\n",
    "\n",
    "features = torch.tensor(x_data[random_video_id], dtype=torch.float)\n",
    "caption = generate_caption(model, features, tokenizer)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83a1b64a-a568-4673-8782-2628cdf101c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random video ID: tHLiYTS9Iz8_1_16\n",
      "Generated caption: a boy is playing with a ball\n",
      "Random video ID: 2mUMTFnQWaw_1_9\n",
      "Generated caption: a man is riding a horse\n",
      "Random video ID: ZvJvNcukZ4w_0_10\n",
      "Generated caption: a <unk> is eating\n",
      "Random video ID: eZLxohGP4IE_15_25\n",
      "Generated caption: a man is cutting a <unk>\n",
      "Random video ID: 04Gt01vatkk_308_321\n",
      "Generated caption: a woman is slicing an onion\n",
      "Random video ID: HZ-BuDDmvVk_0_10\n",
      "Generated caption: a woman is <unk> a small animal\n",
      "Random video ID: 5x_OGEdO6Z8_0_21\n",
      "Generated caption: a man is <unk> a <unk> on a <unk>\n",
      "Random video ID: kWLNZzuo3do_24_31\n",
      "Generated caption: a woman is cutting a <unk>\n",
      "Random video ID: nULE40HEWpA_5_11\n",
      "Generated caption: a woman is playing with a <unk>\n",
      "Random video ID: 3SKyc0aKx70_46_52\n",
      "Generated caption: a man is playing a <unk>\n"
     ]
    }
   ],
   "source": [
    "input_size = 4096\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = VideoCaptioningModel(EncoderLSTM(input_size, hidden_size), DecoderLSTM(embed_size, hidden_size, vocab_size)).to(device)\n",
    "model.load_state_dict(torch.load('video_caption_model_1.pth', map_location=device))\n",
    "\n",
    "for _ in range(10):\n",
    "    random_video_id = random.choice(test_corpus['VideoID'].unique())\n",
    "    print(f\"Random video ID: {random_video_id}\")\n",
    "    \n",
    "    features = torch.tensor(x_data[random_video_id], dtype=torch.float)\n",
    "    caption = generate_caption(model, features, tokenizer)\n",
    "    print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde2adb9-c50b-4830-b8d4-64a1bb24f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random video ID: gGDtPJzh_0s_30_45\n",
      "Generated caption: a person is slicing a piece of bread\n",
      "Random video ID: lv8d_qLLqsk_1_20\n",
      "Generated caption: a <unk> <unk> <unk> <unk> <unk> <unk> <unk> to be <unk> to be singing\n",
      "Random video ID: io2dbV-Qbus_215_247\n",
      "Generated caption: a man is cutting something\n",
      "Random video ID: 08pVpBq706k_175_212\n",
      "Generated caption: a cat is playing with a <unk> of white\n",
      "Random video ID: dtwXtwJByYk_5_14\n",
      "Generated caption: a man is <unk>\n",
      "Random video ID: 08pVpBq706k_175_212\n",
      "Generated caption: a cat is playing with a <unk> of white\n",
      "Random video ID: nTasT5h0LEg_12_14\n",
      "Generated caption: a funny animal\n",
      "Random video ID: HO_ovIrLWfQ_1_11\n",
      "Generated caption: a woman is swimming in a water\n",
      "Random video ID: 9BScZRpF7SI_31_36\n",
      "Generated caption: a man is playing with <unk>\n",
      "Random video ID: c_XV7nPoRg8_2_12\n",
      "Generated caption: a man is playing a <unk> on the <unk>\n"
     ]
    }
   ],
   "source": [
    "input_size = 4096\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = VideoCaptioningModel(EncoderLSTM(input_size, hidden_size), DecoderLSTM(embed_size, hidden_size, vocab_size)).to(device)\n",
    "model.load_state_dict(torch.load('video_caption_model_4.pth', map_location=device))\n",
    "\n",
    "for _ in range(10):\n",
    "    random_video_id = random.choice(test_corpus['VideoID'].unique())\n",
    "    print(f\"Random video ID: {random_video_id}\")\n",
    "    \n",
    "    features = torch.tensor(x_data[random_video_id], dtype=torch.float)\n",
    "    caption = generate_caption(model, features, tokenizer)\n",
    "    print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c3ac6f84-0e41-4ff8-9c0e-04fcb65953b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoID                                         8MVo7fje_oE_130_136\n",
       "Description       <bos> a man drains water out of a container of...\n",
       "PaddedSequence    [3, 2, 7, 1, 32, 57, 14, 2, 319, 14, 336, 4, 0...\n",
       "Name: 62927, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_corpus.iloc[62927]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4ccbd-5be6-44bb-a04d-afcc6ad01a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b5abe44-1455-411b-b21a-b1a7056f5dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def evaluate_bleu_score(model, test_corpus, x_data, tokenizer, device):\n",
    "    model.eval()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    total_bleu = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    for i, row in test_corpus.iterrows():\n",
    "        video_id = row['VideoID']\n",
    "        if video_id not in x_data:\n",
    "            continue\n",
    "\n",
    "        # features = torch.tensor(x_data[video_id][:30], dtype=torch.float).unsqueeze(0).to(device)  # crop to 30 frames\n",
    "        features = torch.tensor(x_data[video_id], dtype=torch.float).unsqueeze(0).to(device)\n",
    "        generated = generate_caption(model, features.squeeze(0), tokenizer)  # returns a sentence string\n",
    "\n",
    "        # Tokenize both predicted and reference sentences\n",
    "        reference = word_tokenize(row['Description'].replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").strip().lower())\n",
    "        candidate = word_tokenize(generated.strip().lower())\n",
    "\n",
    "        # Compute BLEU score (unigram + bigram)\n",
    "        bleu_score = sentence_bleu([reference], candidate, smoothing_function=smoothie, weights=(0.5, 0.5))\n",
    "        total_bleu += bleu_score\n",
    "        n_samples += 1\n",
    "\n",
    "    avg_bleu = total_bleu / max(n_samples, 1)\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a527a5ee-98e7-4d93-85a0-cb2393b78b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score on test set using model1: 0.1734\n",
      "Average BLEU score on test set using model2: 0.1685\n",
      "Average BLEU score on test set using model3: 0.1586\n",
      "Average BLEU score on test set using model3: 0.1548\n"
     ]
    }
   ],
   "source": [
    "model1 = VideoCaptioningModel(EncoderLSTM(input_size, hidden_size),\n",
    "                               DecoderLSTM(embed_size, hidden_size, vocab_size)).to(device)\n",
    "\n",
    "model2 = VideoCaptioningModel(EncoderLSTM(input_size, hidden_size),\n",
    "                               DecoderLSTM(embed_size, hidden_size, vocab_size)).to(device)\n",
    "\n",
    "model3 = VideoCaptioningModel(EncoderLSTM(input_size, hidden_size),\n",
    "                               DecoderLSTM(embed_size, hidden_size, vocab_size)).to(device)\n",
    "\n",
    "model4 = VideoCaptioningModel(EncoderLSTM(input_size, hidden_size),\n",
    "                               DecoderLSTM(embed_size, hidden_size, vocab_size)).to(device)\n",
    "\n",
    "model1.load_state_dict(torch.load('video_caption_model_1.pth', map_location=device))\n",
    "model2.load_state_dict(torch.load('video_caption_model_2.pth', map_location=device))\n",
    "model3.load_state_dict(torch.load('video_caption_model_3.pth', map_location=device))\n",
    "model4.load_state_dict(torch.load('video_caption_model_4.pth', map_location=device))\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "\n",
    "bleu1 = evaluate_bleu_score(model1, test_corpus, x_data, tokenizer, device)\n",
    "bleu2 = evaluate_bleu_score(model2, test_corpus, x_data, tokenizer, device)\n",
    "bleu3 = evaluate_bleu_score(model3, test_corpus, x_data, tokenizer, device)\n",
    "bleu4 = evaluate_bleu_score(model4, test_corpus, x_data, tokenizer, device)\n",
    "\n",
    "print(f\"Average BLEU score on test set using model1: {bleu1:.4f}\")\n",
    "print(f\"Average BLEU score on test set using model2: {bleu2:.4f}\")\n",
    "print(f\"Average BLEU score on test set using model3: {bleu3:.4f}\")\n",
    "print(f\"Average BLEU score on test set using model3: {bleu4:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
